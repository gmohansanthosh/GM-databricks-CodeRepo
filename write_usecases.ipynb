{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd79aa32-4a39-4fd2-be75-9c95a3bb2cfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###6. Write Operations (Data Conversion/Schema migration) - CSV Format Usecases\n",
    "1. Write customer data into CSV format using overwrite mode\n",
    "2. Write usage data into CSV format using append mode\n",
    "3. Write tower data into CSV format with header enabled and custom separator (|)\n",
    "4. Read the tower data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88cea3ee-8d81-490f-b0f2-69f205d6c1bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Pre-requisite for 6.1 : The Data should be read & loaded into the memory using Read option - spark.read.csv.\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "custom_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"customer_age\", IntegerType(), True),\n",
    "    StructField(\"customer_city\", StringType(), True),\n",
    "    StructField(\"customer_plan_type\", StringType(), True)])\n",
    "\n",
    "read_customer_df = spark.read.schema(custom_schema).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "display(read_customer_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "502e62b5-f9e0-4de4-8eca-afa837b7f558",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#6.1.Write customer data into CSV format using overwrite mode.\n",
    "\n",
    "write_customer_csv_df_ovr = read_customer_df.write.options(header='true').mode(\"overwrite\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/csvout/\")\n",
    "#display(write_customer_csv_df_ovr)\n",
    "spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/csvout/\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9db119aa-1099-4b37-9d23-e344875aa045",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pre-requisite for 6.2 : Read the Usage data & display.\n",
    "\n",
    "read_usage_df = spark.read.csv(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\", sep='\\t', header=True)\n",
    "display(read_usage_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a26dc672-23d6-4fba-8b25-52135deeb0b4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 4"
    }
   },
   "outputs": [],
   "source": [
    "#6.2 Write usage data into CSV format using append mode.\n",
    "#pre-requisite : Read the Usage data & display.\n",
    "write_usage_df_append = read_usage_df.select(\"customer_id\",\"sms_count\").write.options(header='true').mode(\"append\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usagewrite1.csv\")\n",
    "#display(write_usage_df_append)\n",
    "spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usagewrite1.csv\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d451f926-5cc9-40b0-97b2-aec5c500efb5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 6"
    }
   },
   "outputs": [],
   "source": [
    "#Pre-requisite of 6.3 - Read Tower Data & display.\n",
    "read_tower_df = spark.read.csv(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region*\",\n",
    "    header=True,\n",
    "    sep=\"|\",\n",
    "    pathGlobFilter=\"*.csv\"\n",
    "    #inferSchema=True,\n",
    "    #recursiveFileLookup=True\n",
    ")\n",
    "display(read_tower_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d4607d3-c780-4e64-90d2-97c476a9023f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 7"
    }
   },
   "outputs": [],
   "source": [
    "#6.3 Write tower data into CSV format with header enabled and custom separator (|)\n",
    "Write_Tower_df = read_tower_df.write.options(header='true', sep='|').mode(\"overwrite\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_logs_regionwrite\")\n",
    "spark.read.format(\"csv\").option(\"header\", \"true\").option(\"sep\", \"|\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_logs_regionwrite\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebd94917-d7fd-4d93-acf4-4494936ba1d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#6.4 Read the tower data in a dataframe and show only 5 rows.\n",
    "read_tower_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf56f4bd-394d-40c1-8dfe-ec9182702ae3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#6.5 Download the file into local from the catalog volume location and see the data of any of the above files opening in a notepad++.\n",
    "\n",
    "#All 3 files are able to open & viewable in Notepad++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1b322a6-e86c-4c29-95bf-3aceea82d55d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###7. Write Operations (Data Conversion/Schema migration - JSON Format Usecases\n",
    "1.Write customer data into JSON format using overwrite mode<br>\n",
    "2.Write usage data into JSON format using append mode and snappy compression format<br>\n",
    "3.Write tower data into JSON format using ignore mode and observe the behavior of this mode<br>\n",
    "4.Read the tower data in a dataframe and show only 5 rows.<br>\n",
    "5.Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54cd25f1-61d8-454a-a131-7b528ec36442",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769352682483}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Cell 11"
    }
   },
   "outputs": [],
   "source": [
    "#Pre-requisite of 7.1 - Read Data & Display.\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "custom_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"customer_age\", IntegerType(), True),\n",
    "    StructField(\"customer_city\", StringType(), True),\n",
    "    StructField(\"customer_plan_type\", StringType(), True)])\n",
    "\n",
    "read_customer_df = spark.read.schema(custom_schema).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "display(read_customer_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f7d6c9d-a6a0-4325-9f7c-1b0b2e11b5b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#7.1.Write customer data into JSON format using overwrite mode.\n",
    "\n",
    "read_customer_df.write.json(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/jsonout\",mode=\"overwrite\")\n",
    "#display(write_customer_csv_df_ovr)\n",
    "spark.read.format(\"json\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/jsonout\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a06183c-7900-46f2-a586-2958175e374d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Pre-requisite of 7.2 - Read the Usages Data & Display.\n",
    "\n",
    "read_usage_df = spark.read.csv(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\", sep='\\t', header=True)\n",
    "display(read_usage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb3a6a13-971b-43ee-87aa-7fe1d4fea391",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#7.2.Write usage data into JSON format using append mode and snappy compression format.\n",
    "\n",
    "write_usage_json_df = read_usage_df.write.mode(\"append\").option(\"compression\",\"snappy\").json(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/jsonout/\")\n",
    "display(write_usage_json_df)\n",
    "spark.read.format(\"json\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/jsonout\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6c945b0-6525-43a1-b869-5491ec8e8ebd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#7.4 Read the tower data in a dataframe and show only 5 rows.\n",
    "read_tower_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bf8e58e-9cdb-4e23-a701-7403f952489d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#5.Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++.\n",
    "\n",
    "#yes, I have opened all 3 files, out of which 2 awere opened & usages files is compressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d9bd28e-459a-47ca-85af-75a9f483d76f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###8. Write Operations (Data Conversion/Schema migration) – Parquet Format Usecases\n",
    "1. Write customer data into Parquet format using overwrite mode and in a gzip format\n",
    "2. Write usage data into Parquet format using error mode\n",
    "3. Write tower data into Parquet format with gzip compression option\n",
    "4. Read the usage data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96eca18a-9d12-448d-b7f5-f287f221c3ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Pre-requisite of 8.1 - Read Data & Display.\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "custom_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"customer_age\", IntegerType(), True),\n",
    "    StructField(\"customer_city\", StringType(), True),\n",
    "    StructField(\"customer_plan_type\", StringType(), True)])\n",
    "\n",
    "read_customer_df = spark.read.schema(custom_schema).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "display(read_customer_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f9c6376-be96-4ca3-9a3f-6e9749a331fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#8.1. Write customer data into Parquet format using overwrite mode and in a gzip format\n",
    "write_customer_parquet_df = read_customer_df.write.mode(\"overwrite\").option(\"compression\",\"gzip\").parquet(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/parquetout/\")\n",
    "display(write_customer_parquet_df)\n",
    "spark.read.format(\"parquet\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/parquetout/\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79bb94b7-3abe-40e2-94e5-b0bc2952f23a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Pre-requisite of 8.2 Read usage data & Display.\n",
    "\n",
    "read_usage_df = spark.read.csv(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\", sep='\\t', header=True)\n",
    "display(read_usage_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "428a9220-2848-45bd-bcad-306dc72cd6f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#8.2 Write usage data into Parquet format using error mode.\n",
    "\n",
    "write_usage_parquet_df = read_usage_df.write.mode(\"error\").parquet(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/parquetout/\")\n",
    "display(write_usage_parquet_df)\n",
    "spark.read.format(\"parquet\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/parquetout/\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4d2e51a-dadc-44db-b8ee-886f11e04348",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Pre-requisite of 8.3 - Read Tower vdata & Display.\n",
    "\n",
    "read_tower_df = spark.read.options(header='true',sep='|',inferSchema='true',pathGlobeFilter='.csv',recursiveFileLookup='true').format('csv').load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region*\")\n",
    "display(read_tower_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f74b015-1746-41b6-b6bb-e2f5bf73cbc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#8.3 - Write tower data into Parquet format with gzip compression option\n",
    "\n",
    "write_tower_parquet_df = read_tower_df.write.mode(\"overwrite\").option(\"compression\",\"gzip\").parquet(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/parquetout/\")\n",
    "display(write_tower_parquet_df)\n",
    "spark.read.format(\"parquet\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/parquetout/\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84ce6942-8e3a-4ea3-b7cf-38868edccfb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#8.4 - Read the usage data in a dataframe and show only 5 rows.\n",
    "display(read_usage_df.limit(5))\n",
    "\n",
    "\n",
    "#8.5 - Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++.\n",
    "\n",
    "#All files are Compressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b151ce81-1981-48e4-b8e9-34e373ce1b55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###9. Write Operations (Data Conversion/Schema migration) - Orc Format Usecases\n",
    "1. Write customer data into ORC format using overwrite mode. <br>\n",
    "2. Write usage data into ORC format using append mode. <br>\n",
    "3. Write tower data into ORC format and see the output file structure. <br>\n",
    "4. Read the usage data in a dataframe and show only 5 rows.<br>\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ab238e4-6359-4552-8367-16d838243896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Pre-requisite of 9.1 - Read Customer & Display.\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "custom_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"customer_age\", IntegerType(), True),\n",
    "    StructField(\"customer_city\", StringType(), True),\n",
    "    StructField(\"customer_plan_type\", StringType(), True)])\n",
    "\n",
    "read_customer_df = spark.read.schema(custom_schema).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "display(read_customer_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d6b42e5-b096-46c8-88ed-a81ef8c56a1f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 27"
    }
   },
   "outputs": [],
   "source": [
    "#9.1 - Write customer data into ORC format using overwrite mode.\n",
    "\n",
    "write_customer_orc_df = read_customer_df.write.mode(\"overwrite\").orc(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/orcout/\")\n",
    "display(write_customer_orc_df)\n",
    "spark.read.format(\"orc\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/orcout/\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84cde71e-25cb-4ed5-8d81-83d2de14d278",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Pre-requisite of 9.2 - Read Usage Data & Display.\n",
    "read_usage_df = spark.read.csv(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\", sep='\\t', header=True)\n",
    "display(read_usage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2291ac8-6eaf-45b0-8cbd-b7d95f5d9461",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#9.2 - Write usage data into ORC format using append mode.\n",
    "\n",
    "write_usage_orc_df = read_usage_df.write.mode(\"append\").orc(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/orcout/\")\n",
    "display(write_usage_orc_df)\n",
    "spark.read.format(\"orc\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/orcout/\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33f8c5ad-193c-449b-b4b8-51f5342d2340",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pre-requisite of 9.3 - Read Tower Data & Display.\n",
    "read_tower_df = spark.read.options(header='true',sep='|',inferSchema='true',pathGlobeFilter='.csv',recursiveFileLookup='true').format('csv').load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region*\")\n",
    "display(read_tower_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ce9ed66-97aa-4376-a770-04eff0190702",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 9.3 -  Write tower data into ORC format and see the output file structure.\n",
    "\n",
    "write_tower_orc_df = read_tower_df.write.mode(\"overwrite\").orc(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/orcout/\")\n",
    "display(write_tower_orc_df)\n",
    "spark.read.format(\"orc\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/orcout/\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cc99adc-c1ef-4514-aa52-f746a9829fdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 9.4 - Read the usage data in a dataframe and show only 5 rows.\n",
    "display(read_usage_df.limit(5))\n",
    "# 9.5 - Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++.\n",
    "# Answer - Yes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdd12c9a-3336-4c35-86fb-69f8e72fd3a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###10. Write Operations (Data Conversion/Schema migration) – Delta Format Usecases\n",
    "1. Write customer data into Delta format using overwrite mode.<br>\n",
    "2. Write usage data into Delta format using append mode. <br>\n",
    "3. Write tower data into Delta format and see the output file structure. <br>\n",
    "4. Read the usage data in a dataframe and show only 5 rows.<br>\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++.<br>\n",
    "6. Compare the parquet location and delta location and try to understand what is the differentiating factor, as both are parquet files only.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0857c767-7436-41bb-8657-f4ddfb9f9e05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pre-requisite of 10.1 - Read Customer & Display.\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "custom_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"customer_age\", IntegerType(), True),\n",
    "    StructField(\"customer_city\", StringType(), True),\n",
    "    StructField(\"customer_plan_type\", StringType(), True)])\n",
    "\n",
    "read_customer_df = spark.read.schema(custom_schema).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "display(read_customer_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b0c99e7-2ad0-469c-ba94-a57742717ffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 10.1 - Write customer data into Delta format using overwrite mode.\n",
    "\n",
    "write_customer_delta_df = read_customer_df.write.mode(\"overwrite\").format('delta').save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/deltaout/\")\n",
    "display(write_customer_delta_df)\n",
    "spark.read.format(\"delta\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/deltaout/\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36cd9246-02a9-4c9a-b24c-ee7989de4b5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pre-requisite of 10.2 - Read Usage Data & Display.\n",
    "\n",
    "read_usage_df = spark.read.csv(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\", sep='\\t', header=True)\n",
    "display(read_usage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fa2a2f6-ef5b-4739-8e16-b9521173e23d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 10.2 - Write usage data into Delta format using append mode.\n",
    "\n",
    "write_usage_delta_df = read_usage_df.write.mode(\"append\").format('delta').save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/deltaout/\")\n",
    "display(write_usage_delta_df)\n",
    "spark.read.format(\"delta\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/deltaout/\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bce8923c-8597-4c43-8092-c6b47eb4c875",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pre-requisite of 10.3 - Read Tower Data & Display.\n",
    "read_tower_df = spark.read.options(header='true',sep='|',inferSchema='true',pathGlobeFilter='.csv',recursiveFileLookup='true').format('csv').load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region*\")\n",
    "display(read_tower_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deb723a9-7c11-4db8-b1a0-acd544ab0de1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 10.3 - Write tower data into Delta format and see the output file structure.\n",
    "\n",
    "write_tower_delta_df = read_tower_df.write.mode(\"overwrite\").option(\"compression\",\"gzip\").format('delta').save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/deltaout/\")\n",
    "display(write_tower_delta_df)\n",
    "spark.read.format(\"delta\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/deltaout/\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "422cea89-28dc-4afd-88ea-d9fd5d6087b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 10.4 - Read the usage data in a dataframe and show only 5 rows.\n",
    "display(read_usage_df.limit(5))\n",
    "# 10.5 - Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++.\n",
    "# Answer - Yes, I can see the delta log which is not present in orc and parquet files.\n",
    "\n",
    "# 10.6 - Compare the parquet location and delta location and try to understand what is the differentiating factor, as both are parquet files only.\n",
    "# Answer - Both are Parquet files only, but the delta format holds a log folder which comprises the history of the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29619ea6-8304-4bba-9f9b-f120603a1a18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##11. Write Operations (Lakehouse Usecases) – Delta table Usecases\n",
    "1. Write customer data using saveAsTable() as a managed table\n",
    "2. Write usage data using saveAsTable() with overwrite mode\n",
    "3. Drop the managed table and verify data removal\n",
    "4. Go and check the table overview and realize it is in delta format in the Catalog.\n",
    "5. Use spark.read.sql to write some simple queries on the above tables created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caa98890-17d6-42af-a4a2-c822792515ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 11.1 - Write customer data using saveAsTable() as a managed table\n",
    "\n",
    "read_customer_df= spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\").toDF(\"customer_id\",\"customer_name\",\"customer_age\",\"customer_city\",\"customer_plan_type\")\n",
    "display(read_customer_df)\n",
    "\n",
    "read_customer_df.write.format('delta').mode('overwrite').saveAsTable(\"telecom_catalog_assign.landing_zone.customer1_table\")\n",
    "spark.sql(\"select * from telecom_catalog_assign.landing_zone.customer1_table\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e93e19c-54f0-4123-93ff-08cc68ab9551",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 11.2.Write usage data using saveAsTable() with overwrite mode\n",
    "\n",
    "read_usage_df= spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\",header='true',inferSchema='True',sep='\\t')\n",
    "\n",
    "write_usage_df = read_usage_df.write.mode('overwrite').format('delta').saveAsTable(\"telecom_catalog_assign.landing_zone.usage_table\")\n",
    "display(write_usage_df)\n",
    "spark.sql(\"select * from telecom_catalog_assign.landing_zone.usage_table\").display()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71061e3d-7016-47df-ad9f-7ab30d7984b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 11.3 - Drop the managed table and verify data removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a90a11a8-7a13-409e-b942-e8e3e0e0b158",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "11.3 - Drop the managed table and verify data removal"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE telecom_catalog_assign.landing_zone.customer_table;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d46c29c8-ce6a-446d-8a6d-3c7ff5db340c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 11.4 - Go and check the table overview and realize it is in delta format in the Catalog.\n",
    "\n",
    "# Answer - \"Yes, the table is stored as a delta format by default in the data source.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90859963-22c5-4031-896f-729f4a568245",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 11.5 - Use spark.read.sql to write some simple queries on the above tables created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "824ab93d-aec9-41a2-8b5d-8ff0e8e129e9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Filter tables with correct .where() usage"
    }
   },
   "outputs": [],
   "source": [
    "usage_table = spark.sql(\"select * from telecom_catalog_assign.landing_zone.usage_table\").where(\"data_mb > 1000\")\n",
    "customer1_table = spark.sql(\"select * from telecom_catalog_assign.landing_zone.customer1_table\").where(\"customer_plan_type = 'PREPAID'\")\n",
    "display(customer1_table)\n",
    "display(usage_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6687e62c-f94b-4cfe-937c-6a6278e2cfe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##12. Write Operations (Lakehouse Usecases) – Delta table Usecases\n",
    "1. Write customer data using insertInto() in a new table and find the behavior\n",
    "2. Write usage data using insertTable() with overwrite mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bcd10d7-ce33-465b-83c6-739080b41776",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 12.1 - Write customer data using insertInto() in a new table and find the behavior\n",
    "\n",
    "read_customer_df= spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\").toDF(\"customer_id\",\"customer_name\",\"customer_age\",\"customer_city\",\"customer_plan_type\")\n",
    "\n",
    "write_customer_df = read_customer_df.write.mode('overwrite').format('delta').insertInto(\"telecom_catalog_assign.landing_zone.customer1_table\")\n",
    "write_sql=spark.sql(\"select * from telecom_catalog_assign.landing_zone.customer1_table\")\n",
    "display(write_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e6c8db4-6036-4f30-a9ef-882159f8217b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 12.2 - Write usage data using insertTable() with overwrite mode\n",
    "\n",
    "read_usage_df= spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\",header='true',inferSchema='True',sep='\\t')\n",
    "\n",
    "write_usage_df = read_usage_df.write.mode('overwrite').format('delta').insertInto(\"telecom_catalog_assign.landing_zone.usage_table\")\n",
    "display(write_usage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e30f1e3a-28a2-47ed-a782-6f7261d1ae12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "911ad5f2-baa0-447a-80b6-4626f3f488ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##13. Write Operations (Lakehouse Usecases) – Delta table Usecases\n",
    "1. Write customer data into XML format using rowTag as cust\n",
    "2. Write usage data into XML format using overwrite mode with the rowTag as usage\n",
    "3. Download the xml data and open the file in notepad++ and see how the xml file looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f682ad1f-def6-4fb1-a42a-a6462e2c2fcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 13.1.Write customer data into XML format using rowTag as cust\n",
    "\n",
    "read_customer_df= spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\").toDF(\"customer_id\",\"customer_name\",\"customer_age\",\"customer_city\",\"customer_plan_type\")\n",
    "\n",
    "write_customer_df = read_customer_df.write.mode('overwrite').format('xml').option(\"rowTag\",\"cust\").save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/xmlout\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b499f1d-6313-40b5-b032-46f246083044",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 13.2.Write usage data into XML format using overwrite mode with the rowTag as usage\n",
    "\n",
    "read_usage_df= spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\",header='true',inferSchema='True',sep='\\t')\n",
    "\n",
    "write_usage_df = read_usage_df.write.mode('overwrite').format('xml').options(rowTag=\"usage\").save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/xmlout\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43fcf1d5-382d-449c-8442-7837b2252114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 13.3 - Download the xml data and open the file in notepad++ and see how the xml file looks like\n",
    "\n",
    "#I have downloaded both the files and verified in notepad++, it has stored as a xml format with rowtag\"\"\"\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c359c947-1770-4181-9c92-86fe8cdd80a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##14. Compare all the downloaded files (csv, json, orc, parquet, delta and xml) \n",
    "1. Capture the size occupied between all of these file formats and list the formats below based on the order of size from small to big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "509b3359-76d2-4efd-a203-86f7e52e8533",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "14.1 - Minimal fix for Delta column names"
    }
   },
   "outputs": [],
   "source": [
    "# 14.1 - Capture the size occupied between all of these file formats and list the formats below based on the order of size from small to big.\n",
    "\n",
    "read_df = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/compare/txns\",header=True,inferSchema=True)\n",
    "\n",
    "write_df = read_df.write.mode('overwrite').format('csv').save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/compare/csvout\")\n",
    "\n",
    "write_df1 = read_df.write.mode('overwrite').format('orc').save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/compare/orcout\")\n",
    "\n",
    "write_df2 = read_df.write.mode('overwrite').format('parquet').save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/compare/parquetout\")\n",
    "\n",
    "write_df3 = read_df.write.mode('overwrite').format('json').save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/compare/jsonout\")\n",
    "\n",
    " write_df4 = read_df.write.mode('overwrite').format('delta').save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/compare/deltaout\")\n",
    "\n",
    "write_df5 = read_df.write.mode('overwrite').format('xml').options(rowtag=\"cust\").save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/compare/xmlout\")\n",
    "\n",
    "#order from small to large size orc,delta,parquet,csv,json,xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3525b77a-7fe4-4a88-b977-7d66394b479e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##15. Try to do permutation and combination of performing Schema Migration & Data Conversion operations like...\n",
    "1. Read any one of the above orc data in a dataframe and write it to dbfs in a parquet format\n",
    "2. Read any one of the above parquet data in a dataframe and write it to dbfs in a delta format\n",
    "3. Read any one of the above delta data in a dataframe and write it to dbfs in a xml format\n",
    "4. Read any one of the above delta table in a dataframe and write it to dbfs in a json format\n",
    "5. Read any one of the above delta table in a dataframe and write it to another table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70b30184-c1d8-41e8-a588-fe44917b3c1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#15.1 Read any one of the above orc data in a dataframe and write it to dbfs in a parquet format\n",
    "customer_orc_df = spark.read.format(\"orc\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/orcout/\")\n",
    "customer_orc_df.write.mode(\"overwrite\").format(\"parquet\").save(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_orc_to_parquet/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a91246f-4488-47f0-a708-d1cf4880f571",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#15.2 Read any one of the above parquet data in a dataframe and write it to dbfs in a delta format\n",
    "customer_parquet_df = spark.read.format(\"parquet\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/parquetout\")\n",
    "customer_parquet_df.write.mode(\"overwrite\").format(\"delta\").save(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_parquet_to_delta/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b76953b4-c16f-4941-b41f-2d0a2c96c799",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#15.3 Read any one of the above delta data in a dataframe and write it to dbfs in a xml format\n",
    "customer_delta_df = spark.read.format(\"delta\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/deltaout\")\n",
    "customer_delta_df.write.mode(\"overwrite\").format(\"xml\").options(rowtag=\"cust\").save(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_delta_to_xml/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc721bc5-8d00-420c-a963-89bdebef29ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#15.4 Read any one of the above delta table in a dataframe and write it to dbfs in a json format\n",
    "customer_table_df = spark.read.table(\"telecom_catalog_assign.landing_zone.customer1_table\")\n",
    "customer1_table_df.write.mode(\"overwrite\").json(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_delta_table_to_json/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e205a6c9-00d0-4a2d-8b9f-d07db829fee3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#15.4 Read any one of the above delta table in a dataframe and write it to another table\n",
    "customer_table_df = spark.read.table(\"telecom_catalog_assign.landing_zone.customer_table\")\n",
    "customer_table_df.write.mode(\"overwrite\").saveAsTable(\"telecom_catalog_assign.landing_zone.customer_table1\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b026adb-f360-40b4-8640-3ba1c63396cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##16. Do a final exercise of defining one/two liner of... \n",
    "1. When to use/benifits csv\n",
    "2. When to use/benifits json\n",
    "3. When to use/benifit orc\n",
    "4. When to use/benifit parquet\n",
    "5. When to use/benifit delta\n",
    "6. When to use/benifit xml\n",
    "7. When to use/benifit delta tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d682dfe8-dd2e-42b2-a78e-0dcfd3ca9618",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "16.1. CSV – When to use / Benefits\n",
    "\n",
    "Use CSV for simple data exchange and quick viewing because it is human-readable, lightweight, and supported everywhere, but it is not suitable for large-scale processing.\n",
    "\n",
    "16.2. JSON – When to use / Benefits\n",
    "\n",
    "Use JSON for semi-structured data and API integrations because it supports nested and flexible schemas, making it ideal for event and application data.\n",
    "\n",
    "16.3. ORC – When to use / Benefits\n",
    "\n",
    "Use ORC for large data, read-heavy analytical workloads because it provides excellent compression, fast reads, and efficient predicate pushdown.\n",
    "\n",
    "16.4. Parquet – When to use / Benefits\n",
    "\n",
    "Use Parquet for big-data analytics across multiple platforms because it is a columnar, compressed, and widely supported format that improves query performance.\n",
    "\n",
    "16.5. Delta – When to use / Benefits\n",
    "\n",
    "Use Delta for reliable data lake processing because it adds ACID transactions, schema enforcement, time travel, and scalable incremental loads on top of Parquet.\n",
    "\n",
    "16.6. XML – When to use / Benefits\n",
    "\n",
    "Use XML for legacy systems and structured data exchange where strict schemas and hierarchical data representation are required.\n",
    "\n",
    "16.7. Delta Tables – When to use / Benefits\n",
    "\n",
    "Use Delta tables for production-grade lakehouse architectures because they ensure data consistency, versioning, and optimized performance for large datasets."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7699361060417776,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "write_usecases",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
