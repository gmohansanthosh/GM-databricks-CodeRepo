{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4e9db29-71e6-4ee6-a139-a2e6adafec63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Enterprise Fleet Analytics Pipeline: Focuses on the business outcome (analytics) and the domain (fleet/logistics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60102b53-da15-4c74-a306-b675d15c78d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![logistics](logistics_project.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c798b032-39ac-4bf6-9992-78159146fb4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Download the data from the below gdrive and upload into the catalog\n",
    "https://drive.google.com/drive/folders/1J3AVJIPLP7CzT15yJIpSiWXshu1iLXKn?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81ca9127-636a-4be2-ad66-6af734d83aa7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Spark Session."
    }
   },
   "outputs": [],
   "source": [
    "# Import Spark Session.\n",
    "\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73641445-2b65-4138-89f3-44e10449c278",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. Visibily/Manually opening the file and capture couple of data patterns (Manual Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "507bfd1d-8d3d-417b-8da8-074150a9eec3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. Programatically try to find couple of data patterns applying below EDA (File: logistics_source1)\n",
    "1. Apply inferSchema and toDF to create a DF and analyse the actual data.\n",
    "2. Analyse the schema, datatypes, columns etc.,\n",
    "3. Analyse the duplicate records count and summary of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6702b71d-749a-4327-893d-7da58aeaa5ce",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "2.1 - Apply inferSchema and toDF to create a DF and analyse the actual data."
    }
   },
   "outputs": [],
   "source": [
    "# 2.1 - Apply inferSchema and toDF to create a DF and analyse the actual data.\n",
    "\n",
    "df1 = spark.read.options(header= 'True', InferSchema= 'True').csv('/Volumes/prodcatalog/logistics/datalake/logistics_source1.txt')\n",
    "#display(df1)\n",
    "df2 = spark.read.options(header= 'True', InferSchema= 'True').csv('/Volumes/prodcatalog/logistics/datalake/logistics_source2.txt')\n",
    "#df1.count()\n",
    "distinct_count = df1.distinct().count()\n",
    "print(distinct_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ceefb7fb-9120-4dd9-b8c0-1fc23d7d20e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2.2 - Analyse the schema, datatypes, columns etc.,\n",
    "df1.printSchema()\n",
    "display(df1.schema)\n",
    "display(df1.columns)\n",
    "display(df1.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bb5a92b-67af-46dd-a899-5bcd55347b9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2.3 - Analyse the duplicate records count and summary of the dataframe.\n",
    "\n",
    "#print(\"Total count of dataset\",df1.count())\n",
    "print(\"de-duplicated record (all columns) count\",df1.distinct().count())        # distinct will remove entire columns duplicates and give the count of the records\n",
    "print(\"de-duplicated record (all columns) count\",df1.dropDuplicates().count())  # dropDuplicates also will do the same, but we can pass required columns\n",
    "print(\"de-duplicated record (selected columns) count\",df1.dropDuplicates(['shipment_id']).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de27b31f-de9a-457b-be06-00ada4960419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###a. Passive Data Munging -  (File: logistics_source1  and logistics_source2)\n",
    "Without modifying the data, identify:<br>\n",
    "Shipment IDs that appear in both master_v1 and master_v2<br>\n",
    "Records where:<br>\n",
    "1. shipment_id is non-numeric\n",
    "2. age is not an integer<br>\n",
    "\n",
    "Count rows having: <br>\n",
    "3. fewer columns than expected\n",
    "4. more columns than expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ca663ce-06bd-492a-af18-6eb6f819ac4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#path_lgs1=\"/Volumes/prodcatalog/logistics/datalake/logistics_source1.txt\"\n",
    "#path_lgs2=\"/Volumes/prodcatalog/logistics/datalake/logistics_source2.txt\"\n",
    "\n",
    "bothlgs_df = spark.read.csv(path=[\"/Volumes/prodcatalog/logistics/datalake/logistics_source1.txt\",\"/Volumes/prodcatalog/logistics/datalake/logistics_source2.txt\"],header=True,inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e3847e3-df9f-4302-8d5b-c01576cf71bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Passive Data Munging\n",
    "# 2.a.1 - shipment_id is non-numeric\n",
    "\n",
    "# #from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "Non_num_shipid_df = bothlgs_df.filter(~col(\"shipment_id\").rlike(\"^[0-9]+$\")|col(\"shipment_id\").isNull())\n",
    "#display(Non_num_shipid_df)\n",
    "\n",
    "# 2.a.2 - age is not an integer\n",
    "\n",
    "Non_int_age_df = bothlgs_df.filter(~col(\"age\").rlike(\"^[0-9]+$\")|col(\"age\").isNull())\n",
    "#display(Non_int_age_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15be308e-a54b-4b98-8107-83c332a27db2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2.a.3 -  fewer columns than expected.\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField,StringType,ShortType,IntegerType\n",
    " \n",
    "struttype1=StructType([StructField('ship_id', IntegerType(), True), StructField('first_name', StringType(), True), StructField('last_name', StringType(), True), StructField('age', ShortType(), True), StructField('role', StringType(), True),StructField(\"corruptedrows\",StringType())])\n",
    " \n",
    "df_1 = spark.read.schema(struttype1).csv(path=\"/Volumes/prodcatalog/logistics/datalake/logistics_source1.txt\",header=True,mode='permissive',columnNameOfCorruptRecord=\"corruptedrows\")\n",
    "#display(df_1)\n",
    "fewer_df = df_1.where(\"size(split(corruptedrows, ',')) < 5\")\n",
    "#display(fewer_df)\n",
    "\n",
    "# 2.a.4 - more columns than expected\n",
    "\n",
    "more_df = df_1.where(\"size(split(corruptedrows, ',')) > 5\")\n",
    "#display(more_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c81fc736-6e9f-4093-9f40-8e047989b602",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###**b. Active Data Munging** File: logistics_source1 and logistics_source2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7b773a5-b1db-4b1f-bf3e-93067d0483ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####1.Combining Data + Schema Merging (Structuring)\n",
    "1. Read both files without enforcing schema\n",
    "2. Align them into a single canonical schema: shipment_id,\n",
    "first_name,\n",
    "last_name,\n",
    "age,\n",
    "role,\n",
    "hub_location,\n",
    "vehicle_type,\n",
    "data_source\n",
    "3. Add data_source column with values as: system1, system2 in the respective dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85443643-c866-419a-8071-b123a842ce51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2.b.1 - Read both files without enforcing schema\n",
    "\n",
    "rawdatalgs_df1 = spark.read.csv(\"/Volumes/prodcatalog/logistics/datalake/logistics_source1.txt\",header= True)\n",
    "#display(rawdatalgs_df1)\n",
    "rawdatalgs_df2 = spark.read.csv(\"/Volumes/prodcatalog/logistics/datalake/logistics_source2.txt\",header= True)\n",
    "#display(rawdatalgs_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f9b2579-5756-4ed5-9b59-cb4e5aca2982",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 18"
    }
   },
   "outputs": [],
   "source": [
    "# 2.b.3 - Add data_source column with values as: system1, system2 in the respective dataframes.\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "rawdatalgs_df1 = rawdatalgs_df1.withColumn(\"data_source\",lit(\"system1\"))\n",
    "rawdatalgs_df2 = rawdatalgs_df2.withColumn(\"data_source\",lit(\"system2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4713e7a-663c-436b-bf92-1e5e57fcde5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2.b.2 - Align them into a single canonical schema: shipment_id, first_name, last_name, age, role, hub_location, vehicle_type, data_source\n",
    "\n",
    "schemamergedlgs_df = rawdatalgs_df1.unionByName(rawdatalgs_df2,allowMissingColumns = True)\n",
    "# display(schemamergedlgs_df)\n",
    "\n",
    "final_df = schemamergedlgs_df.select(\"shipment_id\",\"first_name\",\"last_name\",\"age\",\"role\",\"hub_location\",\"vehicle_type\",\"data_source\")\n",
    "# display(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "628e4769-0e24-481b-8b5c-33204a91ed3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####2. Cleansing, Scrubbing: \n",
    "Cleansing (removal of unwanted datasets)<br>\n",
    "1. Mandatory Column Check - Drop any record where any of the following columns is NULL:shipment_id, role<br>\n",
    "2. Name Completeness Rule - Drop records where both of the following columns are NULL: first_name, last_name<br>\n",
    "3. Join Readiness Rule - Drop records where the join key is null: shipment_id<br>\n",
    "\n",
    "Scrubbing (convert raw to tidy)<br>\n",
    "4. Age Defaulting Rule - Fill NULL values in the age column with: -1<br>\n",
    "5. Vehicle Type Default Rule - Fill NULL values in the vehicle_type column with: UNKNOWN<br>\n",
    "6. Invalid Age Replacement - Replace the following values in age:\n",
    "\"ten\" to -1\n",
    "\"\" to -1<br>\n",
    "7. Vehicle Type Normalization - Replace inconsistent vehicle types: \n",
    "truck to LMV\n",
    "bike to TwoWheeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2db9c92f-4ccb-40c9-916e-9a64c32e8b06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2.1 - Mandatory Column Check - Drop any record where any of the following columns is NULL:shipment_id, role\n",
    "\n",
    "cleansed_df1 = final_df.na.drop(how=\"any\",subset=[\"shipment_id\",\"role\"])\n",
    "#display(cleansed_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64084e8a-f96e-45d2-abf6-3eca402b7244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2.2 - Name Completeness Rule - Drop records where both of the following columns are NULL: first_name, last_name\n",
    "\n",
    "cleansed_df2 = final_df.na.drop(how=\"all\",subset=[\"first_name\",\"last_name\"])\n",
    "#display(cleansed_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82bb60ec-af31-4ca1-bfa2-bbc21bc083d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2.3 - Join Readiness Rule - Drop records where the join key is null: shipment_id\n",
    "\n",
    "#cleansed_df3 = cleansed_df2.filter(~col(\"shipment_id\").rlike(\"[a-zA-Z]\"))\n",
    "cleansed_df3 = cleansed_df2.where(\"shipment_id not rlike '[a-zA-Z]' AND shipment_id IS NOT NULL\")\n",
    "# display(cleansed_df3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bda4664-5c1b-4842-86d1-d9a92d4e759d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scrubbing (convert raw to tidy)\n",
    "# 2.4. - Age Defaulting Rule - Fill NULL values in the age column with: -1\n",
    "scrubbed_df1 = cleansed_df3.na.fill(-1, subset=[\"age\"]) \n",
    "#display(scrubbed_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d9b1843-3d99-4bed-b58c-383af88f1946",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2.5 - Vehicle Type Default Rule - Fill NULL values in the vehicle_type column with: UNKNOWN\n",
    "scrubbed_df2 = scrubbed_df1.na.fill(\"UNKNOWN\",subset=[\"vehicle_type\"])\n",
    "#display(scrubbed_df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aaccd6c-5de2-4b3f-9053-96f5a0e9e927",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2.6 -  Invalid Age Replacement - Replace the following values in age: \"ten\" to -1 \"\" to -1\n",
    "\n",
    "find_replace = {\"ten\": \"-1\",\"\": \"-1\" }\n",
    "scrubbed_df3 = scrubbed_df2.na.replace(find_replace,subset=['age'])\n",
    "#display(scrubbed_df3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32aa8c2e-0b82-41cd-861e-aaff6b30f6ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2.7. Vehicle Type Normalization - Replace inconsistent vehicle types: truck to LMV bike to TwoWheeler\n",
    "find_replace_2 = {'Truck':'LMV','Bike':'TwoWheeler'}\n",
    "scrubbed_df4 = scrubbed_df3.na.replace(find_replace_2,subset=[\"vehicle_type\"])\n",
    "#display(scrubbed_df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b089e58-4b74-41e5-b050-bbfa8d249467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####3. Standardization, De-Duplication and Replacement / Deletion of Data to make it in a usable format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccc60af7-9398-4de0-93ce-1f5bf9dd5c47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Creating shipments Details data Dataframe creation <br>\n",
    "1. Create a DF by Reading Data from logistics_shipment_detail.json\n",
    "2. As this data is a clean json data, it doesn't require any cleansing or scrubbing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20a34c59-0113-4647-80d0-9190e208bde3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3.1 - Create a DF by Reading Data from logistics_shipment_detail.json\n",
    "# 3.2 - As this data is a clean json data, it doesn't require any cleansing or scrubbing.\n",
    "\n",
    "dfjson1 = spark.read.json(\"/Volumes/prodcatalog/logistics/datalake/logistics_shipment_detail_3000.json\",multiLine=True)\n",
    "#dfjson1.printSchema()\n",
    "display(dfjson1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cd9b438-99be-431d-a81a-493c23b2b998",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Standardizations:<br>\n",
    "\n",
    "1. Add a column<br> \n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>: domain as 'Logistics',  current timestamp 'ingestion_timestamp' and 'False' as 'is_expedited'\n",
    "2. Column Uniformity: \n",
    "role - Convert to lowercase<br>\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2)<br>\n",
    "vehicle_type - Convert values to UPPERCASE<br>\n",
    "Source Files: DF of logistics_shipment_detail_3000.json\n",
    "hub_location - Convert values to initcap case<br>\n",
    "Source Files: DF of merged(logistics_source1 & logistics_source2)<br>\n",
    "3. Format Standardization:<br>\n",
    "Source Files: DF of logistics_shipment_detail_3000.json<br>\n",
    "Convert shipment_date to yyyy-MM-dd<br>\n",
    "Ensure shipment_cost has 2 decimal precision<br>\n",
    "4. Data Type Standardization<br>\n",
    "Standardizing column data types to fix schema drift and enable mathematical operations.<br>\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2) <br>\n",
    "age: Cast String to Integer<br>\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "shipment_weight_kg: Cast to Double<br>\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "is_expedited: Cast to Boolean<br>\n",
    "5. Naming Standardization <br>\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2)<br>\n",
    "Rename: first_name to staff_first_name<br>\n",
    "Rename: last_name to staff_last_name<br>\n",
    "Rename: hub_location to origin_hub_city<br>\n",
    "6. Reordering columns logically in a better standard format:<br>\n",
    "Source File: DF of Data from all 3 files<br>\n",
    "shipment_id (Identifier), staff_first_name (Dimension)staff_last_name (Dimension), role (Dimension), origin_hub_city (Location), shipment_cost (Metric), ingestion_timestamp (Audit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1318489-eb65-47fb-ab41-c8022bd3a30c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import upper,lower,col,lit,initcap,lpad,to_date,when,current_timestamp\n",
    "\n",
    "# Add a column\n",
    "# Source File: DF of logistics_shipment_detail_3000.json\n",
    "# domain as 'Logistics', current timestamp 'ingestion_timestamp' and 'False' as 'is_expedited'\n",
    "\n",
    "add_col_df = dfjson1.withColumn(\"domain\",lit('Logistics'))\n",
    "#display(add_col_df)\n",
    "add_col_df1 = add_col_df.withColumn(\"ingestion_timestamp\",current_timestamp())\n",
    "#display(add_col_df1)\n",
    "expedited_df = add_col_df1.withColumn(\"is_expedited\",lit(\"False\"))\n",
    "#display(expedited_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcd03bc3-769a-41bd-97b2-0176aa8ea0cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" 2. Column Uniformity: role - Convert to lowercase\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2)\n",
    "vehicle_type - Convert values to UPPERCASE\n",
    "Source Files: DF of logistics_shipment_detail_3000.json hub_location - Convert values to initcap case\n",
    "Source Files: DF of merged(logistics_source1 & logistics_source2)\"\"\"\n",
    "\n",
    "col_uni_df = scrubbed_df4.withColumn(\"role\",lower(col(\"role\")))\n",
    "#display(col_uni_df)\n",
    "col_uni_df1= col_uni_df.withColumn('vehicle_type',upper(col('vehicle_type')))\n",
    "#display(col_uni_df1)\n",
    "col_uni_df2 = col_uni_df1.withColumn(\"hub_location\",initcap(col(\"hub_location\")))\n",
    "#display(col_uni_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffa8d7b2-f035-4bc3-8d48-5097099dfc9e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Format Standardization: Fix date parsing error"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Format Standardization:\n",
    "Source Files: DF of logistics_shipment_detail_3000.json\n",
    "Convert shipment_date to yyyy-MM-dd\n",
    "Ensure shipment_cost has 2 decimal precision\"\"\"\n",
    "from pyspark.sql.functions import to_date, col, date_format, try_to_date\n",
    "\n",
    "df = dfjson1.withColumnRenamed(\"shipment_id\",\"shipment_ref\").withColumnRenamed(\"shipment_date\",\"dispatch_date\").withColumnRenamed(\"shipment_cost\",\"delivery_cost\")\n",
    "format_fun_df = df.withColumn(\"shipment_id\",lpad(col(\"shipment_id\").cast(\"string\"),10,\"0\"))\n",
    "display(df)\n",
    "\n",
    "date_conversion_df = df.withColumn( \"dispatch_date\", date_format(to_date(col(\"dispatch_date\"), \"yy-MM-dd\"), \"yyyy/MM/dd\") )\n",
    "display(date_conversion_df)\n",
    "\n",
    "delivery_cost_df = df.withColumn(\"delivery_cost\",col(\"delivery_cost\").cast(\"double\"))\n",
    "display(delivery_cost_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02426138-6661-4ffa-a01f-b69d70ff3ed9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Data Type Standardization\n",
    "Standardizing column data types to fix schema drift and enable mathematical operations.\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2)\n",
    "age: Cast String to Integer\n",
    "Source File: DF of logistics_shipment_detail_3000.json\n",
    "shipment_weight_kg: Cast to Double\n",
    "Source File: DF of logistics_shipment_detail_3000.json\n",
    "is_expedited: Cast to Boolean\"\"\"\n",
    "\n",
    "dtype_df= scrubbed_df4.withColumn(\"shipment_id\",col(\"shipment_id\").cast(\"int\")).withColumn(\"first_name\",col(\"first_name\").cast(\"string\")).withColumn(\"last_name\",col(\"last_name\").cast(\"string\")).withColumn(\"age\",col(\"age\").cast(\"int\"))\n",
    "dtype_df = dtype_df.withColumn(\"role\",col(\"role\").cast(\"string\")).withColumn(\"vehicle_type\",col(\"vehicle_type\").cast(\"string\")).withColumn(\"data_source\",col(\"data_source\").cast(\"string\"))\n",
    "#display(dtype_df)\n",
    "\n",
    "\n",
    "dtype_df2 = dfjson1.withColumn(\"shipment_weight_kg\",col(\"shipment_weight_kg\").cast(\"double\"))\n",
    "#display(dtype_df2)\n",
    "\n",
    "expedited_cast_df = expedited_df.withColumn(\"is_expedited\",when((col(\"shipment_status\")==\"IN_TRANSIT\"),True).otherwise(False))\n",
    "#display(expedited_cast_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1d3abf7-2111-443a-bec7-7951cf14a2fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Naming Standardization\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2)\n",
    "Rename: first_name to staff_first_name\n",
    "Rename: last_name to staff_last_name\n",
    "Rename: hub_location to origin_hub_city\"\"\"\n",
    "\n",
    "naming_df = scrubbed_df4.withColumnsRenamed({'first_name':'staff_first_name','last_name':'staff_last_name','hub_location':'origin_hub_city'})\n",
    "#display(naming_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7952e554-7510-41aa-9dec-1ddfd2938cbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Reordering columns logically in a better standard format:\n",
    "Source File: DF of Data from all 3 files\n",
    "shipment_id (Identifier), staff_first_name (Dimension)staff_last_name (Dimension), role (Dimension), origin_hub_city (Location), shipment_cost (Metric), ingestion_timestamp (Audit)\"\"\"\n",
    "\n",
    "expedited_cast_df = expedited_cast_df.select(\"shipment_id\",\"order_id\",\"cargo_type\",\"vehicle_type\",\"shipment_status\",\"payment_mode\",\"destination_city\",\"source_city\",\"shipment_weight_kg\",\"shipment_cost\",\"shipment_date\",\"domain\",\"is_expedited\",\"ingestion_timestamp\")\n",
    "#display(expedited_cast_df)\n",
    "\n",
    "rawdatalgs_df1 = rawdatalgs_df1.select(\"shipment_id\", \"first_name\",\"last_name\",\"role\",\"age\",\"data_source\")\n",
    "#display(rawdatalgs_df1)\n",
    "\n",
    "rawdatalgs_df2 = rawdatalgs_df2.select(\"shipment_id\", \"first_name\",\"last_name\",\"role\",\"age\",\"vehicle_type\",\"hub_location\",\"data_source\")\n",
    "#display(rawdatalgs_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bafa5c8-7355-4d4f-9caf-40e0e0d303f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Deduplication:\n",
    "1. Apply Record Level De-Duplication\n",
    "2. Apply Column Level De-Duplication (Primary Key Enforcement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e903abb2-be8b-4ddb-b3dd-7d9c2cf34a4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1 - Apply Record Level De-Duplication\n",
    "\n",
    "duplicate_df = dfjson1.dropDuplicates()\n",
    "display(duplicate_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaff110b-63a7-4ef5-aa02-032d393f34a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2 - Apply Column Level De-Duplication (Primary Key Enforcement)\n",
    "\n",
    "col_duplicate_df = dfjson1.dropDuplicates([\"shipment_id\"])\n",
    "display(col_duplicate_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "311d404f-b7d4-4e56-9f09-9367ec05e283",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Adding of Columns (Data Enrichment)\n",
    "*Creating new derived attributes to enhance traceability and analytical capability.*\n",
    "\n",
    "**1. Add Audit Timestamp (`load_dt`)**\n",
    "Source File: DF of logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** We need to track exactly when this record was ingested into our Data Lakehouse for auditing purposes.\n",
    "* **Action:** Add a column `load_dt` using the function `current_timestamp()`.\n",
    "\n",
    "**2. Create Full Name (`full_name`)**\n",
    "Source File: DF of logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** The reporting dashboard requires a single field for the driver's name instead of separate columns.\n",
    "* **Action:** Create `full_name` by concatenating `first_name` and `last_name` with a space separator.\n",
    "* **Result:** \"Rajesh\" + \" \" + \"Kumar\" -> **\"Rajesh Kumar\"**\n",
    "\n",
    "**3. Define Route Segment (`route_segment`)**\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "* **Scenario:** The logistics team wants to analyze performance based on specific transport lanes (Source to Destination).\n",
    "* **Action:** Combine `source_city` and `destination_city` with a hyphen.\n",
    "* **Result:** \"Chennai\" + \"-\" + \"Pune\" -> **\"Chennai-Pune\"**\n",
    "\n",
    "**4. Generate Vehicle Identifier (`vehicle_identifier`)**\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "* **Scenario:** We need a unique tracking code that immediately tells us the vehicle type and the shipment ID.\n",
    "* **Action:** Combine `vehicle_type` and `shipment_id` to create a composite key.\n",
    "* **Result:** \"Truck\" + \"_\" + \"500001\" -> **\"Truck_500001\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "335d75a8-3fa8-48c6-984a-a9b4e99783b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Add Audit Timestamp (load_dt) Source File: logistics_source1 and logistics_source2\n",
    "\n",
    "'''Scenario: We need to track exactly when this record was ingested into our Data Lakehouse for auditing purposes.\n",
    "Action: Add a column load_dt using the function current_timestamp().'''\n",
    "\n",
    "audit_df = scrubbed_df4.withColumn(\"load_dt\",current_timestamp())\n",
    "#display(audit_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b68d663c-3f00-48d6-86df-927745b8ebc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. Create Full Name (full_name) Source File: logistics_source1 and logistics_source2\n",
    "\n",
    "'''Scenario: The reporting dashboard requires a single field for the driver's name instead of separate columns.\n",
    "Action: Create full_name by concatenating first_name and last_name with a space separator.\n",
    "Result: \"Rajesh\" + \" \" + \"Kumar\" -> \"Rajesh Kumar\"'''\n",
    "\n",
    "from pyspark.sql.functions import concat_ws\n",
    "concat_df = audit_df.withColumn('full_name',concat_ws(\" \",col(\"first_name\"),col(\"last_name\")))\n",
    "#display(concat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bb9311e-1297-4ab9-86d1-0413bf55997b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3. Define Route Segment (route_segment) Source File: logistics_shipment_detail_3000.json\n",
    "\n",
    "'''Scenario: The logistics team wants to analyze performance based on specific transport lanes (Source to Destination).\n",
    "Action: Combine source_city and destination_city with a hyphen.\n",
    "Result: \"Chennai\" + \"-\" + \"Pune\" -> \"Chennai-Pune\"'''\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import concat_ws\n",
    "concat_df1 = dfjson1.withColumn('source_destination',concat_ws(\"-\",col(\"source_city\"),col(\"destination_city\")))\n",
    "#display(concat_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3a488bc-c8ed-49f9-b8db-660667ac73ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#4. Generate Vehicle Identifier (vehicle_identifier) Source File: logistics_shipment_detail_3000.json\n",
    "\n",
    "'''Scenario: We need a unique tracking code that immediately tells us the vehicle type and the shipment ID.\n",
    "Action: Combine vehicle_type and shipment_id to create a composite key.\n",
    "Result: \"Truck\" + \"_\" + \"500001\" -> \"Truck_500001\"'''\n",
    "\n",
    "from pyspark.sql.functions import concat_ws\n",
    "concat_df2 = dfjson1.withColumn('tracking_code',concat_ws(\"_\",col(\"vehicle_type\"),col(\"shipment_id\")))\n",
    "#display(concat_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e525645-18d1-4a16-9909-afc89a2ed57d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Deriving of Columns (Time Intelligence)\n",
    "*Extracting temporal features from dates to enable period-based analysis and reporting.*<br>\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "**1. Derive Shipment Year (`shipment_year`)**\n",
    "* **Scenario:** Management needs an annual performance report to compare growth year-over-year.\n",
    "* **Action:** Extract the year component from `shipment_date`.\n",
    "* **Result:** \"2024-04-23\" -> **2024**\n",
    "\n",
    "**2. Derive Shipment Month (`shipment_month`)**\n",
    "* **Scenario:** Analysts want to identify seasonal peaks (e.g., increased volume in December).\n",
    "* **Action:** Extract the month component from `shipment_date`.\n",
    "* **Result:** \"2024-04-23\" -> **4** (April)\n",
    "\n",
    "**3. Flag Weekend Operations (`is_weekend`)**\n",
    "* **Scenario:** The Operations team needs to track shipments handled during weekends to calculate overtime pay or analyze non-business day capacity.\n",
    "* **Action:** Flag as **'True'** if the `shipment_date` falls on a Saturday or Sunday.\n",
    "\n",
    "**4. Flag shipment status (`is_expedited`)**\n",
    "* **Scenario:** The Operations team needs to track shipments is IN_TRANSIT\n",
    " or DELIVERED.\n",
    "* **Action:** Flag as **'True'** if the `shipment_status` IN_TRANSIT or DELIVERED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee33ca4c-702c-416e-975c-9767f6d9b6ed",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "#1. Derive Shipment Year (shipment_year)\n",
    "\n",
    "'''Scenario: Management needs an annual performance report to compare growth year-over-year.\n",
    "Action: Extract the year component from shipment_date.\n",
    "Result: \"2024-04-23\" -> 2024'''\n",
    "\n",
    "from pyspark.sql.functions import year, to_date, col, date_format\n",
    "\n",
    "year_extract_df = dfjson1.withColumn( \"shipment_year\", year(to_date(col(\"shipment_date\"), \"yy-MM-dd\")))\n",
    "           \n",
    "display(year_extract_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c26a4c89-6133-4437-b9f0-c0263a8883c5",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770555524922}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Derive Shipment Month (shipment_month)"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Derive Shipment Month (shipment_month)\n",
    "\n",
    "\"\"\"Scenario: Analysts want to identify seasonal peaks (e.g., increased volume in December).\n",
    "Action: Extract the month component from shipment_date.\n",
    "Result: \"2024-04-23\" -> 4 (April)\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import year, month, date_format, to_date, col\n",
    "\n",
    "year_month_df = year_extract_df.select(\"*\", date_format(to_date(col(\"shipment_date\"), \"yy-MM-dd\"), \"MMMM\").alias(\"shipment_month\"))\n",
    "\n",
    "display(year_month_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d25fc5ed-4b8f-4cbc-9cf3-a2b6df7faf2f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Flag Weekend Operations (is_weekend)"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Flag Weekend Operations (is_weekend)\n",
    "\n",
    "\"\"\"Scenario: The Operations team needs to track shipments handled during weekends to calculate overtime pay or analyze non-business day capacity.\n",
    "Action: Flag as 'True' if the shipment_date falls on a Saturday or Sunday.\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import dayofweek, when, col, to_date\n",
    "flag_df = year_month_df.withColumn(\"Flag\", when(dayofweek(to_date(col('shipment_date'), 'yy-MM-dd')).isin([1,7]), True).otherwise(False))\n",
    "display(flag_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d067397f-693e-4eda-97bf-0dc7ff95e356",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Flag shipment status (is_expedited)\n",
    "\n",
    "\"\"\"Scenario: The Operations team needs to track shipments is IN_TRANSIT or DELIVERED.\n",
    "Action: Flag as 'True' if the shipment_status IN_TRANSIT or DELIVERED.\"\"\"\n",
    "\n",
    "flag_df1 = expedited_cast_df.withColumn(\"is_expedited\",when((col(\"shipment_status\")==\"IN_TRANSIT\") | (col(\"shipment_status\")==\"DELIVERED\"),True).otherwise(False))\n",
    "display(flag_df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f178441f-3675-448e-b8f1-f45336851f82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Enrichment/Business Logics (Calculated Fields)\n",
    "*Deriving new metrics and financial indicators using mathematical and date-based operations.*<br>\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "\n",
    "**1. Calculate Unit Cost (`cost_per_kg`)**\n",
    "* **Scenario:** The Finance team wants to analyze the efficiency of shipments by determining the cost incurred per unit of weight.\n",
    "* **Action:** Divide `shipment_cost` by `shipment_weight_kg`.\n",
    "* **Logic:** `shipment_cost / shipment_weight_kg`\n",
    "\n",
    "**2. Track Shipment Age (`days_since_shipment`)**\n",
    "* **Scenario:** The Operations team needs to monitor how long it has been since a shipment was dispatched to identify potential delays.\n",
    "* **Action:** Calculate the difference in days between the `current_date` and the `shipment_date`.\n",
    "* **Logic:** `datediff(current_date(), shipment_date)`\n",
    "\n",
    "**3. Compute Tax Liability (`tax_amount`)**\n",
    "* **Scenario:** For invoicing and compliance, we must calculate the Goods and Services Tax (GST) applicable to each shipment.\n",
    "* **Action:** Calculate 18% GST on the total `shipment_cost`.\n",
    "* **Logic:** `shipment_cost * 0.18`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8746d5cf-c1d2-411b-ab5d-de4d2638121d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Calculate Unit Cost (cost_per_kg)"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Calculate Unit Cost (cost_per_kg)\n",
    "\n",
    "\"\"\"Scenario: The Finance team wants to analyze the efficiency of shipments by determining the cost incurred per unit of weight.\n",
    "Action: Divide shipment_cost by shipment_weight_kg.\n",
    "Logic: shipment_cost / shipment_weight_kg\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import round, try_divide\n",
    "cost_unit_of_weight = expedited_cast_df.withColumn(\"cost_per_unit\", round(try_divide(col(\"shipment_cost\"), col(\"shipment_weight_kg\")), 2))\n",
    "display(cost_unit_of_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef3944e2-e637-48f8-bd99-32d373872cd0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Track Shipment Age (days_since_shipment)"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Track Shipment Age (days_since_shipment)\n",
    "\n",
    "\"\"\"Scenario: The Operations team needs to monitor how long it has been since a shipment was dispatched to identify potential delays.\n",
    "Action: Calculate the difference in days between the current_date and the shipment_date.\n",
    "Logic: datediff(current_date(), shipment_date)\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import datediff, current_date, to_date, col\n",
    "\n",
    "days_diff_df = cost_unit_of_weight.withColumn(\"delay_days\",\n",
    "    datediff(current_date(), to_date(col(\"shipment_date\"), \"yy-MM-dd\"))\n",
    ")\n",
    "display(days_diff_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42ede382-c541-409f-9acc-2d55fd196f81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Compute Tax Liability (tax_amount)\n",
    "\n",
    "\"\"\"Scenario: For invoicing and compliance, we must calculate the Goods and Services Tax (GST) applicable to each shipment.\n",
    "Action: Calculate 18% GST on the total shipment_cost.\n",
    "Logic: shipment_cost * 0.18\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import round\n",
    "tax_calc_df = days_diff_df.withColumn(\"tax_cost\",round(col(\"shipment_cost\")*0.18,2))\n",
    "display(tax_calc_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9de85d4f-d903-46fd-b110-1476a2383d6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Remove/Eliminate (drop, select, selectExpr)\n",
    "*Excluding unnecessary or redundant columns to optimize storage and privacy.*<br>\n",
    "Source File: DF of logistics_source1 and logistics_source2<br>\n",
    "\n",
    "**1. Remove Redundant Name Columns**\n",
    "* **Scenario:** Since we have already created the `full_name` column in the Enrichment step, the individual name columns are now redundant and clutter the dataset.\n",
    "* **Action:** Drop the `first_name` and `last_name` columns.\n",
    "* **Logic:** `df.drop(\"first_name\", \"last_name\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c05999a-0f3a-4111-9074-ba4cc7f0a649",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770561794269}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "drop_first_last_name_df = concat_df.drop(\"first_name\",\"last_name\")\n",
    "source2_df = drop_first_last_name_df.select(\"shipment_id\",\"full_name\",\"role\",\"age\",\"hub_location\",\"vehicle_type\",\"data_source\",\"load_dt\")\n",
    "display(drop_first_last_name_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7682d4f7-a188-4f86-b60f-c1afa5db220f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Splitting & Merging/Melting of Columns\n",
    "*Reshaping columns to extract hidden values or combine fields for better analysis.*<br>\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "**1. Splitting (Extraction)**\n",
    "*Breaking one column into multiple to isolate key information.*\n",
    "* **Split Order Code:**\n",
    "  * **Action:** Split `order_id` (\"ORD100000\") into two new columns:\n",
    "    * `order_prefix` (\"ORD\")\n",
    "    * `order_sequence` (\"100000\")\n",
    "* **Split Date:**\n",
    "  * **Action:** Split `shipment_date` into three separate columns for partitioning:\n",
    "    * `ship_year` (2024)\n",
    "    * `ship_month` (4)\n",
    "    * `ship_day` (23)\n",
    "\n",
    "**2. Merging (Concatenation)**\n",
    "*Combining multiple columns into a single unique identifier or description.*\n",
    "* **Create Route ID:**\n",
    "  * **Action:** Merge `source_city` (\"Chennai\") and `destination_city` (\"Pune\") to create a descriptive route key:\n",
    "    * `route_lane` (\"Chennai->Pune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b5ba9fd-26e3-45c1-bf77-7bbf7019630d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Spliting.\n",
    "\n",
    "\"\"\"Split Order Code:\n",
    "Action: Split order_id (\"ORD100000\") into two new columns:\n",
    "order_prefix (\"ORD\")\n",
    "order_sequence (\"100000\")\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import split,substring\n",
    "\n",
    "split_df = tax_calc_df.withColumn(\"order_prefix\",substring(col(\"order_id\"),1,3)).withColumn(\"order_sequence\",substring(col(\"order_id\"),4,9))\n",
    "display(split_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1236af18-0aa3-42a1-8c32-b4b739e39ca5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Split shipment_date into year, month, day"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Split Date:\n",
    "Action: Split shipment_date into three separate columns for partitioning:\n",
    "ship_year (2024)\n",
    "ship_month (4)\n",
    "ship_day (23)\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import year, month, dayofmonth, to_date, col\n",
    "split_df1 = split_df.withColumn(\"ship_year\", year(to_date(col(\"shipment_date\"), \"yy-MM-dd\")))\\\n",
    "    .withColumn(\"ship_month\", month(to_date(col(\"shipment_date\"), \"yy-MM-dd\")))\\\n",
    "    .withColumn(\"ship_day\", dayofmonth(to_date(col(\"shipment_date\"), \"yy-MM-dd\")))\n",
    "display(split_df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfd1a54e-26a5-41d6-bdd8-65b4a5b99d97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Merging (Concatenation) Combining multiple columns into a single unique identifier or description.\n",
    "\n",
    "\"\"\"Create Route ID:\n",
    "Action: Merge source_city (\"Chennai\") and destination_city (\"Pune\") to create a descriptive route key:\n",
    "route_lane (\"Chennai->Pune\")\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import concat_ws\n",
    "route_id_df = split_df1. withColumn(\"route_lane\",concat_ws(\"->\",\"source_city\",\"destination_city\"))\n",
    "display(route_id_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3a4891e-0c24-40b4-8ed8-b124efed02f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Data Customization & Processing - Application of Tailored Business Specific Rules\n",
    "\n",
    "### **UDF1: Complex Incentive Calculation**\n",
    "**Scenario:** The Logistics Head wants to calculate a \"Performance Bonus\" for drivers based on tenure and role complexity.\n",
    "\n",
    "**Action:** Create a Python function `calculate_bonus(role, age)` and register it as a Spark UDF.\n",
    "\n",
    "**Logic:**\n",
    "* **IF** `Role` == 'Driver' **AND** `Age` > 50:\n",
    "  * `Bonus` = 15% of Salary (Reward for Seniority)\n",
    "* **IF** `Role` == 'Driver' **AND** `Age` < 30:\n",
    "  * `Bonus` = 5% of Salary (Encouragement for Juniors)\n",
    "* **ELSE**:\n",
    "  * `Bonus` = 0\n",
    "\n",
    "**Result:** A new derived column `projected_bonus` is generated for every row in the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### **UDF2: PII Masking (Privacy Compliance)**\n",
    "**Scenario:** For the analytics dashboard, we must hide the full identity of the staff to comply with privacy laws (GDPR/DPDP), while keeping names recognizable for internal managers.\n",
    "\n",
    "**Business Rule:** Show the first 2 letters, mask the middle characters with `****`, and show the last letter.\n",
    "\n",
    "**Action:** Create a UDF `mask_identity(name)`.\n",
    "\n",
    "**Example:**\n",
    "* **Input:** `\"Rajesh\"`\n",
    "* **Output:** `\"Ra****h\"`\n",
    "<br>\n",
    "**Note: Convert the above udf logic to inbult function based transformation to ensure the performance is improved.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70f1cb15-8d17-4a9d-a69e-aa2f03a146cf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "UDF 1: calculate_bonus with None handling"
    }
   },
   "outputs": [],
   "source": [
    "#UDF1: Complex Incentive Calculation\n",
    "'''Scenario: The Logistics Head wants to calculate a \"Performance Bonus\" for drivers based on tenure and role complexity.\n",
    "Action: Create a Python function calculate_bonus(role, age) and register it as a Spark UDF.\n",
    "Logic:\n",
    "IF Role == 'Driver' AND Age > 50:\n",
    "Bonus = 15% of Salary (Reward for Seniority)\n",
    "IF Role == 'Driver' AND Age < 30:\n",
    "Bonus = 5% of Salary (Encouragement for Juniors)\n",
    "ELSE:\n",
    "Bonus = 0\n",
    "Result: A new derived column projected_bonus is generated for every row in the dataset.'''\n",
    "\n",
    "def calculate_bonus(role,age):\n",
    "    if age is None:\n",
    "        return 0\n",
    "    age = int(age)      # The age is string datatype in given dataset,that's why we are typecasting the datatype while writing function\n",
    "    if  role == 'Driver' and age > 50:\n",
    "        return 0.15\n",
    "    elif role == 'Driver' and age < 30:\n",
    "        return 0.05\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "#UDF2: PII Masking (Privacy Compliance)\n",
    "\"\"\"Scenario: For the analytics dashboard, we must hide the full identity of the staff to comply with privacy laws (GDPR/DPDP), while keeping names recognizable for internal managers.\n",
    "\n",
    "Business Rule: Show the first 2 letters, mask the middle characters with ****, and show the last letter.\n",
    "\n",
    "Action: Create a UDF mask_identity(name).\n",
    "\n",
    "Example:\n",
    "\n",
    "Input: \"Rajesh\"\n",
    "Output: \"Ra****h\"\n",
    "**Note: Convert the above udf logic to inbult function based transformation to ensure the performance is improved.**\"\"\"\n",
    "\n",
    "def mask_identity(name):\n",
    "    return name[0:2]+\"****\"+name[-1]\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "bonusfn= udf(calculate_bonus)\n",
    "perf_bonus_df = source2_df.withColumn(\"projected_bonus\",bonusfn(col(\"role\"),col(\"age\")))\n",
    "display(perf_bonus_df)\n",
    "\n",
    "maskfn = udf(mask_identity)\n",
    "mask_identity_df = perf_bonus_df.withColumn(\"name\",maskfn(col(\"full_name\")))\n",
    "display(mask_identity_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ed3d160-f54e-4bf3-bb2c-40ae7ff1b7d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Data Core Curation & Processing (Pre-Wrangling)\n",
    "*Applying business logic to focus, filter, and summarize data before final analysis.*\n",
    "\n",
    "**1. Select (Projection)**<br>\n",
    "Source Files: DF of logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** The Driver App team only needs location data, not sensitive HR info.\n",
    "* **Action:** Select only `first_name`, `role`, and `hub_location`.\n",
    "\n",
    "**2. Filter (Selection)**<br>\n",
    "Source File: DF of json<br>\n",
    "* **Scenario:** We need a report on active operational problems.\n",
    "* **Action:** Filter rows where `shipment_status` is **'DELAYED'** or **'RETURNED'**.\n",
    "* **Scenario:** Insurance audit for senior staff.\n",
    "* **Action:** Filter rows where `age > 50`.\n",
    "\n",
    "**3. Derive Flags & Columns (Business Logic)**<br>\n",
    "Source File: DF of json<br>\n",
    "* **Scenario:** Identify high-value shipments for security tracking.\n",
    "* **Action:** Create flag `is_high_value` = **True** if `shipment_cost > 50,000`.\n",
    "* **Scenario:** Flag weekend operations for overtime calculation.\n",
    "* **Action:** Create flag `is_weekend` = **True** if day is Saturday or Sunday.\n",
    "\n",
    "**4. Format (Standardization)**<br>\n",
    "Source File: DF of json<br>\n",
    "* **Scenario:** Finance requires readable currency formats.\n",
    "* **Action:** Format `shipment_cost` to string like **\"30,695.80\"**.\n",
    "* **Scenario:** Standardize city names for reporting.\n",
    "* **Action:** Format `source_city` to Uppercase (e.g., \"chennai\"  **\"CHENNAI\"**).\n",
    "\n",
    "**5. Group & Aggregate (Summarization)**<br>\n",
    "Source Files: DF of logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** Regional staffing analysis.\n",
    "* **Action:** Group by `hub_location` and **Count** the number of staff.\n",
    "* **Scenario:** Fleet capacity analysis.\n",
    "* **Action:** Group by `vehicle_type` and **Sum** the `shipment_weight_kg`.\n",
    "\n",
    "**6. Sorting (Ordering)**<br>\n",
    "Source File: DF of json<br>\n",
    "* **Scenario:** Prioritize the most expensive shipments.\n",
    "* **Action:** Sort by `shipment_cost` in **Descending** order.\n",
    "* **Scenario:** Organize daily dispatch schedule.\n",
    "* **Action:** Sort by `shipment_date` (Ascending).\n",
    "\n",
    "**7. Limit (Top-N Analysis)**<br>\n",
    "Source File: DF of json<br>\n",
    "* **Scenario:** Dashboard snapshot of critical delays.\n",
    "* **Action:** Filter for 'DELAYED', Sort by Cost, and **Limit to top 10** rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9992924-2d11-4cfa-b8fa-5c96bf6d0475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Data Wrangling - Transformation & Analytics\n",
    "*Combining, modeling, and analyzing data to answer complex business questions.*\n",
    "\n",
    "### **1. Joins**\n",
    "Source Files:<br>\n",
    "Left Side (staff_df):<br> DF of logistics_source1 & logistics_source2<br>\n",
    "Right Side (shipments_df):<br> DF of logistics_shipment_detail_3000.json<br>\n",
    "#### **1.1 Frequently Used Simple Joins (Inner, Left)**\n",
    "* **Inner Join (Performance Analysis):**\n",
    "  * **Scenario:** We only want to analyze *completed work*. Connect Staff to the Shipments they handled.\n",
    "  * **Action:** Join `staff_df` and `shipments_df` on `shipment_id`.\n",
    "  * **Result:** Returns only rows where a staff member is assigned to a valid shipment.\n",
    "* **Left Join (Idle Resource check):**\n",
    "  * **Scenario:** Find out which staff members are currently *idle* (not assigned to any shipment).\n",
    "  * **Action:** Join `staff_df` (Left) with `shipments_df` (Right) on `shipment_id`. Filter where `shipments_df.shipment_id` is NULL.\n",
    "\n",
    "#### **1.2 Infrequent Simple Joins (Self, Right, Full, Cartesian)**\n",
    "* **Self Join (Peer Finding):**\n",
    "  * **Scenario:** Find all pairs of employees working in the same `hub_location`.\n",
    "  * **Action:** Join `staff_df` to itself on `hub_location`, filtering where `staff_id_A != staff_id_B`.\n",
    "* **Right Join (Orphan Data Check):**\n",
    "  * **Scenario:** Identify shipments in the system that have *no valid driver* assigned (Data Integrity Issue).\n",
    "  * **Action:** Join `staff_df` (Left) with `shipments_df` (Right). Focus on NULLs on the left side.\n",
    "* **Full Outer Join (Reconciliation):**\n",
    "  * **Scenario:** A complete audit to find *both* idle drivers AND unassigned shipments in one view.\n",
    "  * **Action:** Perform a Full Outer Join on `shipment_id`.\n",
    "* **Cartesian/Cross Join (Capacity Planning):**\n",
    "  * **Scenario:** Generate a schedule of *every possible* driver assignment to *every* pending shipment to run an optimization algorithm.\n",
    "  * **Action:** Cross Join `drivers_df` and `pending_shipments_df`.\n",
    "\n",
    "#### **1.3 Advanced Joins (Semi and Anti)**\n",
    "* **Left Semi Join (Existence Check):**\n",
    "  * **Scenario:** \"Show me the details of Drivers who have *at least one* shipment.\" (Standard filtering).\n",
    "  * **Action:** `staff_df.join(shipments_df, \"shipment_id\", \"left_semi\")`.\n",
    "  * **Benefit:** Performance optimization; it stops scanning the right table once a match is found.\n",
    "* **Left Anti Join (Negation Check):**\n",
    "  * **Scenario:** \"Show me the details of Drivers who have *never* touched a shipment.\"\n",
    "  * **Action:** `staff_df.join(shipments_df, \"shipment_id\", \"left_anti\")`.\n",
    "\n",
    "### **2. Lookup**<br>\n",
    "Source File: DF of logistics_source1 and logistics_source2 (merged into Staff DF) and Master_City_List.csv<br>\n",
    "* **Scenario:** Validation. Check if the `hub_location` in the staff file exists in the dataframe of corporate `Master_City_List.csv`.\n",
    "* **Action:** Compare values against this Master_City_List list.\n",
    "\n",
    "### **3. Lookup & Enrichment**<br>\n",
    "Source File: DF of logistics_source1 and logistics_source2 (merged into Staff DF) and Master_City_List.csv dataframe<br>\n",
    "* **Scenario:** Geo-Tagging.\n",
    "* **Action:** Lookup `hub_location` (eg. \"Pune\") in a Master Latitude/Longitude Master_City_List.csv dataframe and enrich our logistics_source (merged dataframe) by adding `lat` and `long` columns for map plotting.\n",
    "\n",
    "### **4. Schema Modeling (Denormalization)**<br>\n",
    "Source Files: DF of All 3 Files (logistics_source1, logistics_source2, logistics_shipment_detail_3000.json)<br>\n",
    "* **Scenario:** Creating a \"Gold Layer\" Table for PowerBI/Tableau.\n",
    "* **Action:** Flatten the Star Schema. Join `Staff`, `Shipments`, and `Vehicle_Master` into one wide table (`wide_shipment_history`) so analysts don't have to perform joins during reporting.\n",
    "\n",
    "### **5. Windowing (Ranking & Trends)**<br>\n",
    "Source Files:<br>\n",
    "DF of logistics_source2: Provides hub_location (Partition Key).<br>\n",
    "logistics_shipment_detail_3000.json: Provides shipment_cost (Ordering Key)<br>\n",
    "* **Scenario:** \"Who are the Top 3 Drivers by Cost in *each* Hub?\"\n",
    "* **Action:**\n",
    "  1. Partition by `hub_location`.\n",
    "  2. Order by `total_shipment_cost` Descending.\n",
    "  3. Apply `dense_rank()` and `row_number()\n",
    "  4. Filter where `rank or row_number <= 3`.\n",
    "\n",
    "### **6. Analytical Functions (Lead/Lag)**<br>\n",
    "Source File: <br>\n",
    "DF of logistics_shipment_detail_3000.json<br>\n",
    "* **Scenario:** Idle Time Analysis.\n",
    "* **Action:** For each driver, calculate the days elapsed since their *previous* shipment.\n",
    "\n",
    "### **7. Set Operations**<br>\n",
    "Source Files: DF of logistics_source1 and logistics_source2<br>\n",
    "* **Union:** Combining `Source1` (Legacy) and `Source2` (Modern) into one dataset (Already done in Active Munging).\n",
    "* **Intersect:** Identifying Staff IDs that appear in *both* Source 1 and Source 2 (Duplicate/Migration Check).\n",
    "* **Except (Difference):** Identifying Staff IDs present in Source 2 but *missing* from Source 1 (New Hires).\n",
    "\n",
    "### **8. Grouping & Aggregations (Advanced)**<br>\n",
    "Source Files:<br>\n",
    "DF of logistics_source2: Provides hub_location and vehicle_type (Grouping Dimensions).<br>\n",
    "DF of logistics_shipment_detail_3000.json: Provides shipment_cost (Aggregation Metric).<br>\n",
    "* **Scenario:** The CFO wants a subtotal report at multiple levels:\n",
    "  1. Total Cost by Hub.\n",
    "  2. Total Cost by Hub AND Vehicle Type.\n",
    "  3. Grand Total.\n",
    "* **Action:** Use `cube(\"hub_location\", \"vehicle_type\")` or `rollup()` to generate all these subtotals in a single query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fbdbf31-7a73-44f2-8e74-6c6fcf35d916",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##6. Data Persistance (LOAD)-> Data Publishing & Consumption<br>\n",
    "\n",
    "Store the inner joined, lookup and enrichment, Schema Modeling, windowing, analytical functions, set operations, grouping and aggregation data into the delta tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aee7e8d0-1fbb-4a6c-af39-316374a94a67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##7.Take the copy of the above notebook and try to write the equivalent SQL for which ever applicable."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BB2_Usecase2_DSL_SQL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
